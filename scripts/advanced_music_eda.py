"""
TICKET-024: é«˜åº¦éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿EDAãƒ»å•é¡Œç‰¹å®šã‚·ã‚¹ãƒ†ãƒ 

éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®å¤–ã‚Œå€¤æ¤œå‡ºã€CV-LBæ ¼å·®è¦å› åˆ†æã€æ¸¬å®šèª¤å·®ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®šã‚’è¡Œã†
ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šã®ãŸã‚ã®åŒ…æ‹¬çš„åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æˆ¦ç•¥è»¢æ›: ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–é™ç•Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ»éŸ³æ¥½ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡é‡è¦–ã¸
ç›®æ¨™: CV-LBæ ¼å·®ï¼ˆ+0.076ï¼‰ã®æ ¹æœ¬åŸå› ç‰¹å®šã¨æ”¹å–„
"""

import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
from loguru import logger

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from scipy import stats
    from sklearn.cluster import DBSCAN
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    VISUALIZATION_AVAILABLE = True
except ImportError as e:
    VISUALIZATION_AVAILABLE = False
    logger.warning(f"å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")

from src.config import FIGURES_DIR, PROCESSED_DATA_DIR


class AdvancedMusicEDA:
    """é«˜åº¦éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿EDAåˆ†æã‚¯ãƒ©ã‚¹."""

    def __init__(self, data_path: Union[str, Path]):
        """åˆæœŸåŒ–.

        Args:
            data_path: åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.data_path = Path(data_path)
        self.data = None

        # 9ã¤ã®åŸºæœ¬ç‰¹å¾´é‡
        self.basic_features = [
            'RhythmScore', 'AudioLoudness', 'VocalContent', 'AcousticQuality',
            'InstrumentalScore', 'LivePerformanceLikelihood', 'MoodScore',
            'TrackDurationMs', 'Energy'
        ]
        self.target = 'BeatsPerMinute'

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.output_dir = FIGURES_DIR / "advanced_eda"
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # çµæœä¿å­˜ç”¨
        self.analysis_results = {}

        # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        if VISUALIZATION_AVAILABLE:
            plt.style.use('default')
            sns.set_palette("husl")

    def load_data(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€.

        Returns:
            èª­ã¿è¾¼ã¿æˆåŠŸã®ãƒ–ãƒ¼ãƒ«å€¤
        """
        try:
            if not self.data_path.exists():
                logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.data_path}")
                return False

            self.data = pd.read_csv(self.data_path)
            logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {self.data.shape}")

            # åŸºæœ¬çµ±è¨ˆè¡¨ç¤º
            logger.info(f"åŸºæœ¬ç‰¹å¾´é‡æ•°: {len(self.basic_features)}")
            logger.info(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {self.target}")
            logger.info(f"BPMç¯„å›²: {self.data[self.target].min():.1f} - {self.data[self.target].max():.1f}")

            return True
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _create_bpm_ranges(self) -> pd.Series:
        """BPMå¸¯åŸŸåˆ†é¡ã‚’ä½œæˆ.

        Returns:
            BPMå¸¯åŸŸãƒ©ãƒ™ãƒ«ã®ã‚·ãƒªãƒ¼ã‚º
        """
        bpm = self.data[self.target]
        conditions = [
            (bpm < 60),
            (bpm >= 60) & (bpm < 80),
            (bpm >= 80) & (bpm < 120),
            (bpm >= 120) & (bpm < 140),
            (bpm >= 140) & (bpm < 180),
            (bpm >= 180)
        ]
        choices = ['Very Slow (<60)', 'Slow (60-80)', 'Moderate (80-120)',
                  'Fast (120-140)', 'Very Fast (140-180)', 'Extreme (180+)']
        return pd.Series(np.select(conditions, choices), index=self.data.index)

    def _create_genre_proxies(self) -> pd.DataFrame:
        """éŸ³æ¥½ã‚¸ãƒ£ãƒ³ãƒ«ä»£ç†å¤‰æ•°ã‚’ä½œæˆ.

        Returns:
            ã‚¸ãƒ£ãƒ³ãƒ«ä»£ç†å¤‰æ•°ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        df = self.data.copy()

        # é›»å­éŸ³æ¥½ç³»ï¼ˆLivePerformanceä½ã€Energyé«˜ï¼‰
        df['Electronic_Proxy'] = (
            (df['LivePerformanceLikelihood'] < 0.3) &
            (df['Energy'] > 0.7)
        ).astype(int)

        # ã‚¢ã‚³ãƒ¼ã‚¹ãƒ†ã‚£ãƒƒã‚¯ç³»ï¼ˆAcousticQualityé«˜ã€InstrumentalScoreé«˜ï¼‰
        df['Acoustic_Proxy'] = (
            (df['AcousticQuality'] > 0.7) &
            (df['InstrumentalScore'] > 0.5)
        ).astype(int)

        # ãƒœãƒ¼ã‚«ãƒ«é‡è¦–ç³»ï¼ˆVocalContenté«˜ï¼‰
        df['Vocal_Heavy'] = (df['VocalContent'] > 0.5).astype(int)

        # ãƒ€ãƒ³ã‚¹ç³»ï¼ˆRhythmScoreé«˜ã€Energyé«˜ï¼‰
        df['Dance_Proxy'] = (
            (df['RhythmScore'] > 0.7) &
            (df['Energy'] > 0.6)
        ).astype(int)

        # ã‚¢ãƒ³ãƒ“ã‚¨ãƒ³ãƒˆç³»ï¼ˆEnergyä½ã€MoodScoreä¸­ç¨‹åº¦ï¼‰
        df['Ambient_Proxy'] = (
            (df['Energy'] < 0.3) &
            (df['MoodScore'] > 0.3) &
            (df['MoodScore'] < 0.7)
        ).astype(int)

        return df

    def analyze_music_outliers(self) -> Dict:
        """éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®å¤–ã‚Œå€¤ã‚’åˆ†æ.

        Returns:
            å¤–ã‚Œå€¤åˆ†æçµæœã®è¾æ›¸
        """
        logger.info("éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®å¤–ã‚Œå€¤åˆ†æã‚’å®Ÿè¡Œä¸­...")

        results = {}

        # 1. BPMç•°å¸¸å€¤æ¤œå‡ºï¼ˆéŸ³æ¥½ç†è«–ãƒ™ãƒ¼ã‚¹ï¼‰
        bpm = self.data[self.target]

        # æ¥µç«¯ãªBPMå€¤ï¼ˆéŸ³æ¥½ç†è«–çš„ã«ç•°å¸¸ï¼‰
        extreme_slow = bpm < 40  # é€šå¸¸ã®æ¥½æ›²ç¯„å›²å¤–
        extreme_fast = bpm > 200  # é€šå¸¸ã®æ¥½æ›²ç¯„å›²å¤–

        results['extreme_bpm'] = {
            'extreme_slow_count': sum(extreme_slow),
            'extreme_fast_count': sum(extreme_fast),
            'extreme_slow_ratio': sum(extreme_slow) / len(bpm),
            'extreme_fast_ratio': sum(extreme_fast) / len(bpm),
            'extreme_slow_samples': bpm[extreme_slow].tolist()[:10],  # æœ€åˆã®10ä»¶
            'extreme_fast_samples': bpm[extreme_fast].tolist()[:10]
        }

        # 2. æ¥½å™¨ãƒãƒ©ãƒ³ã‚¹ç•°å¸¸
        # VocalContentã¨InstrumentalScoreã®çŸ›ç›¾ï¼ˆè«–ç†çš„æ•´åˆæ€§ï¼‰
        vocal_instrumental_conflict = (
            (self.data['VocalContent'] > 0.8) &
            (self.data['InstrumentalScore'] > 0.8)
        )

        # AudioLoudnessã®ç•°å¸¸å€¤ï¼ˆéŸ³éŸ¿å·¥å­¦çš„è¦³ç‚¹ï¼‰
        audio_outliers = (
            (self.data['AudioLoudness'] < -30) |  # ç•°å¸¸ã«å°ã•ã„
            (self.data['AudioLoudness'] > 0)      # 0dBè¶…éï¼ˆç‰©ç†çš„é™ç•Œï¼‰
        )

        results['balance_anomalies'] = {
            'vocal_instrumental_conflict_count': sum(vocal_instrumental_conflict),
            'audio_loudness_outliers_count': sum(audio_outliers),
            'vocal_instrumental_conflict_ratio': sum(vocal_instrumental_conflict) / len(self.data),
            'audio_outlier_ratio': sum(audio_outliers) / len(self.data),
            'audio_outlier_range': {
                'min': self.data['AudioLoudness'].min(),
                'max': self.data['AudioLoudness'].max(),
                'suspicious_low': sum(self.data['AudioLoudness'] < -30),
                'suspicious_high': sum(self.data['AudioLoudness'] > 0)
            }
        }

        # 3. æ¥½æ›²æ§‹é€ ç•°å¸¸
        duration_ms = self.data['TrackDurationMs']
        too_short = duration_ms < 30000  # 30ç§’æœªæº€
        too_long = duration_ms > 600000  # 10åˆ†è¶…é

        results['duration_anomalies'] = {
            'too_short_count': sum(too_short),
            'too_long_count': sum(too_long),
            'too_short_ratio': sum(too_short) / len(duration_ms),
            'too_long_ratio': sum(too_long) / len(duration_ms),
            'duration_stats': {
                'mean_seconds': duration_ms.mean() / 1000,
                'median_seconds': duration_ms.median() / 1000,
                'min_seconds': duration_ms.min() / 1000,
                'max_seconds': duration_ms.max() / 1000
            }
        }

        # 4. ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»ãƒ ãƒ¼ãƒ‰ä¸æ•´åˆ
        energy_mood_mismatch = (
            ((self.data['Energy'] > 0.8) & (self.data['MoodScore'] < 0.3)) |  # é«˜ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»ä½ãƒ ãƒ¼ãƒ‰
            ((self.data['Energy'] < 0.2) & (self.data['MoodScore'] > 0.7))    # ä½ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»é«˜ãƒ ãƒ¼ãƒ‰
        )

        results['energy_mood_mismatch'] = {
            'count': sum(energy_mood_mismatch),
            'ratio': sum(energy_mood_mismatch) / len(self.data)
        }

        logger.success(f"éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿å¤–ã‚Œå€¤åˆ†æå®Œäº† - å•é¡Œãƒ‡ãƒ¼ã‚¿æ¯”ç‡: {sum(extreme_slow) + sum(extreme_fast) + sum(vocal_instrumental_conflict)}/{len(self.data)} ({((sum(extreme_slow) + sum(extreme_fast) + sum(vocal_instrumental_conflict))/len(self.data)*100):.2f}%)")
        return results

    def analyze_cv_lb_gap_factors(self) -> Dict:
        """CV-LBæ ¼å·®è¦å› ã‚’çµ±è¨ˆåˆ†æ.

        Returns:
            CV-LBæ ¼å·®åˆ†æçµæœã®è¾æ›¸
        """
        logger.info("CV-LBæ ¼å·®è¦å› ã®çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œä¸­...")

        results = {}

        # 1. ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®æ­ªåº¦ãƒ»å°–åº¦åˆ†æï¼ˆCV-LBä¸ä¸€è‡´ã®åŸå› ï¼‰
        skewness_analysis = {}
        kurtosis_analysis = {}

        for feature in self.basic_features + [self.target]:
            if feature in self.data.columns:
                feature_data = self.data[feature].dropna()
                skew = stats.skew(feature_data)
                kurt = stats.kurtosis(feature_data)
                skewness_analysis[feature] = skew
                kurtosis_analysis[feature] = kurt

        # é«˜ã„æ­ªåº¦/å°–åº¦ã®ç‰¹å¾´é‡ã‚’ç‰¹å®š
        high_skew_features = {k: v for k, v in skewness_analysis.items() if abs(v) > 1.0}
        high_kurt_features = {k: v for k, v in kurtosis_analysis.items() if abs(v) > 3.0}

        results['distribution_analysis'] = {
            'skewness': skewness_analysis,
            'kurtosis': kurtosis_analysis,
            'high_skew_features': high_skew_features,
            'high_kurtosis_features': high_kurt_features
        }

        # 2. ç‰¹å¾´é‡é–“ã®éç·šå½¢é–¢ä¿‚æ¤œå‡º
        nonlinear_relationships = {}

        for feature in self.basic_features:
            if feature in self.data.columns:
                # ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢ vs ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ã®å·®ï¼ˆéç·šå½¢é–¢ä¿‚ã®æŒ‡æ¨™ï¼‰
                pearson_corr = self.data[feature].corr(self.data[self.target])
                spearman_corr = self.data[feature].corr(self.data[self.target], method='spearman')
                nonlinearity_score = abs(spearman_corr - pearson_corr)

                nonlinear_relationships[feature] = {
                    'pearson': pearson_corr,
                    'spearman': spearman_corr,
                    'nonlinearity_score': nonlinearity_score
                }

        # éç·šå½¢æ€§ãŒé«˜ã„ç‰¹å¾´é‡ã‚’ç‰¹å®š
        high_nonlinear = {k: v for k, v in nonlinear_relationships.items()
                         if v['nonlinearity_score'] > 0.1}

        results['nonlinear_relationships'] = nonlinear_relationships
        results['high_nonlinear_features'] = high_nonlinear

        # 3. BPMç¯„å›²åˆ¥ãƒ‡ãƒ¼ã‚¿å¯†åº¦ã®ä¸å‡ä¸€æ€§
        bpm_ranges = self._create_bpm_ranges()
        density_analysis = bpm_ranges.value_counts(normalize=True).to_dict()

        # å¯†åº¦ã®åã‚Šã‚’è¨ˆç®—
        density_values = list(density_analysis.values())
        density_variance = np.var(density_values)

        results['bpm_density_distribution'] = {
            'distribution': density_analysis,
            'density_variance': density_variance,
            'most_common_range': max(density_analysis, key=density_analysis.get),
            'least_common_range': min(density_analysis, key=density_analysis.get)
        }

        # 4. æ¡ä»¶ä»˜ãåˆ†æ•£åˆ†æï¼ˆç•°åˆ†æ•£æ€§æ¤œå‡ºï¼‰
        heteroscedasticity_analysis = {}

        for feature in self.basic_features:
            if feature in self.data.columns:
                # BPMå¸¯åŸŸåˆ¥ã®ç‰¹å¾´é‡åˆ†æ•£
                variances_by_bpm = []
                for bpm_range in bpm_ranges.unique():
                    mask = bpm_ranges == bpm_range
                    if sum(mask) > 10:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«æ•°
                        variance = self.data.loc[mask, feature].var()
                        variances_by_bpm.append(variance)

                if len(variances_by_bpm) > 1:
                    variance_ratio = max(variances_by_bpm) / min(variances_by_bpm)
                    heteroscedasticity_analysis[feature] = {
                        'variance_ratio': variance_ratio,
                        'max_variance': max(variances_by_bpm),
                        'min_variance': min(variances_by_bpm)
                    }

        results['heteroscedasticity'] = heteroscedasticity_analysis

        logger.success(f"CV-LBæ ¼å·®è¦å› åˆ†æå®Œäº† - é«˜æ­ªåº¦ç‰¹å¾´é‡: {len(high_skew_features)}å€‹, é«˜éç·šå½¢ç‰¹å¾´é‡: {len(high_nonlinear)}å€‹")
        return results

    def analyze_bpm_genre_distribution(self) -> Dict:
        """ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ãƒ»BPMå¸¯åˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’èª¿æŸ».

        Returns:
            åˆ†å¸ƒèª¿æŸ»çµæœã®è¾æ›¸
        """
        logger.info("ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ãƒ»BPMå¸¯åˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒèª¿æŸ»ã‚’å®Ÿè¡Œä¸­...")

        # ã‚¸ãƒ£ãƒ³ãƒ«ä»£ç†å¤‰æ•°ä½œæˆ
        genre_data = self._create_genre_proxies()
        bpm_ranges = self._create_bpm_ranges()
        genre_data['BPM_Range'] = bpm_ranges

        results = {}

        # 1. ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥BPMçµ±è¨ˆ
        genre_columns = ['Electronic_Proxy', 'Acoustic_Proxy', 'Vocal_Heavy',
                        'Dance_Proxy', 'Ambient_Proxy']

        genre_bpm_stats = {}
        genre_counts = {}

        for genre in genre_columns:
            genre_mask = genre_data[genre] == 1
            genre_count = sum(genre_mask)
            genre_counts[genre] = genre_count

            if genre_count > 10:
                bpm_subset = genre_data.loc[genre_mask, self.target]
                genre_bpm_stats[genre] = {
                    'count': len(bpm_subset),
                    'mean': bpm_subset.mean(),
                    'std': bpm_subset.std(),
                    'median': bpm_subset.median(),
                    'min': bpm_subset.min(),
                    'max': bpm_subset.max(),
                    'skewness': stats.skew(bpm_subset),
                    'kurtosis': stats.kurtosis(bpm_subset)
                }

        results['genre_bpm_statistics'] = genre_bpm_stats
        results['genre_counts'] = genre_counts

        # 2. BPMå¸¯åŸŸåˆ¥ç‰¹å¾´é‡çµ±è¨ˆ
        bpm_range_stats = {}
        for bpm_range in bpm_ranges.unique():
            mask = bpm_ranges == bpm_range
            range_count = sum(mask)

            if range_count > 10:
                subset_stats = {'count': range_count}
                for feature in self.basic_features:
                    if feature in genre_data.columns:
                        feature_subset = genre_data.loc[mask, feature]
                        subset_stats[feature] = {
                            'mean': feature_subset.mean(),
                            'std': feature_subset.std(),
                            'median': feature_subset.median()
                        }
                bpm_range_stats[bpm_range] = subset_stats

        results['bpm_range_feature_stats'] = bpm_range_stats

        # 3. ã‚¸ãƒ£ãƒ³ãƒ«Ã—BPMå¸¯åŸŸã®ã‚¯ãƒ­ã‚¹åˆ†å¸ƒ
        cross_distribution = {}
        for genre in genre_columns:
            if genre_counts[genre] > 0:
                cross_dist = pd.crosstab(
                    genre_data['BPM_Range'],
                    genre_data[genre],
                    normalize='index'
                )
                if 1 in cross_dist.columns:
                    cross_distribution[genre] = cross_dist[1].to_dict()

        results['genre_bpm_cross_distribution'] = cross_distribution

        logger.success(f"ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ãƒ»BPMå¸¯åˆ¥åˆ†å¸ƒèª¿æŸ»å®Œäº† - æ¤œå‡ºã‚¸ãƒ£ãƒ³ãƒ«æ•°: {len([g for g, c in genre_counts.items() if c > 10])}")
        return results

    def detect_measurement_errors(self) -> Dict:
        """æ¸¬å®šèª¤å·®ãƒ»è¨˜éŒ²ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š.

        Returns:
            ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºçµæœã®è¾æ›¸
        """
        logger.info("æ¸¬å®šèª¤å·®ãƒ»è¨˜éŒ²ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®šã‚’å®Ÿè¡Œä¸­...")

        results = {}

        # 1. æ•°å€¤ã®ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        numerical_anomalies = {}

        for feature in self.basic_features:
            if feature in self.data.columns:
                feature_data = self.data[feature].dropna()

                # åŒä¸€å€¤ã®ç•°å¸¸ãªé›†ä¸­ï¼ˆè¨˜éŒ²ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ï¼‰
                value_counts = feature_data.value_counts()
                max_count = value_counts.max()
                dominant_value_ratio = max_count / len(feature_data)

                # 0ã®ç•°å¸¸ãªå¤šã•
                zero_count = sum(feature_data == 0)
                zero_ratio = zero_count / len(feature_data)

                # è² ã®å€¤ï¼ˆä¸€éƒ¨ç‰¹å¾´é‡ã§ã¯ç•°å¸¸ï¼‰
                negative_count = sum(feature_data < 0) if feature != 'AudioLoudness' else 0

                numerical_anomalies[feature] = {
                    'dominant_value_ratio': dominant_value_ratio,
                    'most_common_value': value_counts.index[0],
                    'most_common_count': max_count,
                    'zero_count': zero_count,
                    'zero_ratio': zero_ratio,
                    'negative_count': negative_count,
                    'unique_values': len(value_counts)
                }

        results['numerical_anomalies'] = numerical_anomalies

        # 2. ç¯„å›²å¤–å€¤æ¤œå‡ºï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ï¼‰
        domain_violations = {}

        # ç¢ºç‡çš„ç‰¹å¾´é‡ï¼ˆ0-1ç¯„å›²ï¼‰ã®æ¤œè¨¼
        probability_features = ['RhythmScore', 'VocalContent', 'LivePerformanceLikelihood',
                               'MoodScore', 'Energy']
        for feature in probability_features:
            if feature in self.data.columns:
                out_of_range_low = sum(self.data[feature] < 0)
                out_of_range_high = sum(self.data[feature] > 1)
                out_of_range_total = out_of_range_low + out_of_range_high

                domain_violations[feature] = {
                    'out_of_range_low': out_of_range_low,
                    'out_of_range_high': out_of_range_high,
                    'out_of_range_total': out_of_range_total,
                    'out_of_range_ratio': out_of_range_total / len(self.data)
                }

        # AudioLoudness ã®å¦¥å½“æ€§ï¼ˆé€šå¸¸ -60dB ã‹ã‚‰ 0dBï¼‰
        if 'AudioLoudness' in self.data.columns:
            suspicious_low = sum(self.data['AudioLoudness'] < -60)
            suspicious_high = sum(self.data['AudioLoudness'] > 0)
            domain_violations['AudioLoudness'] = {
                'suspicious_low': suspicious_low,
                'suspicious_high': suspicious_high,
                'suspicious_total': suspicious_low + suspicious_high,
                'suspicious_ratio': (suspicious_low + suspicious_high) / len(self.data)
            }

        results['domain_violations'] = domain_violations

        # 3. è«–ç†çš„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        logical_inconsistencies = {}

        # InstrumentalScore + VocalContent ã®åˆè¨ˆãƒã‚§ãƒƒã‚¯
        if 'InstrumentalScore' in self.data.columns and 'VocalContent' in self.data.columns:
            instrumental_vocal_sum = self.data['InstrumentalScore'] + self.data['VocalContent']
            impossible_combinations = sum(instrumental_vocal_sum > 1.2)  # ä½™è£•ã‚’ã‚‚ã£ã¦1.2

            logical_inconsistencies['instrumental_vocal_sum_violation'] = {
                'impossible_count': impossible_combinations,
                'impossible_ratio': impossible_combinations / len(self.data),
                'max_sum': instrumental_vocal_sum.max(),
                'mean_sum': instrumental_vocal_sum.mean()
            }

        results['logical_inconsistencies'] = logical_inconsistencies

        # 4. ç•°å¸¸ãªç›¸é–¢ãƒ‘ã‚¿ãƒ¼ãƒ³
        correlation_anomalies = {}
        correlation_matrix = self.data[self.basic_features].corr()

        # æ¥µã‚ã¦é«˜ã„ç›¸é–¢ï¼ˆ>0.95ï¼‰ã®æ¤œå‡º
        high_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i + 1, len(correlation_matrix.columns)):
                corr_val = abs(correlation_matrix.iloc[i, j])
                if corr_val > 0.95:
                    high_correlations.append({
                        'feature1': correlation_matrix.columns[i],
                        'feature2': correlation_matrix.columns[j],
                        'correlation': correlation_matrix.iloc[i, j]
                    })

        correlation_anomalies['extremely_high_correlations'] = high_correlations
        correlation_anomalies['high_correlation_count'] = len(high_correlations)

        results['correlation_anomalies'] = correlation_anomalies

        # å•é¡Œã®ç·è¨ˆç®—
        total_domain_violations = sum(
            data['out_of_range_total'] if 'out_of_range_total' in data
            else data.get('suspicious_total', 0)
            for data in domain_violations.values()
        )

        logger.success(f"æ¸¬å®šèª¤å·®ãƒ»è¨˜éŒ²ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®šå®Œäº† - ãƒ‰ãƒ¡ã‚¤ãƒ³é•åç·æ•°: {total_domain_violations}")
        return results

    def create_comprehensive_visualizations(self, analysis_results: Dict) -> List[Path]:
        """åŒ…æ‹¬çš„ãªå¯è¦–åŒ–ã‚’ä½œæˆ.

        Args:
            analysis_results: åˆ†æçµæœã®è¾æ›¸

        Returns:
            ä½œæˆã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        if not VISUALIZATION_AVAILABLE:
            logger.warning("å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return []

        logger.info("åŒ…æ‹¬çš„ãªå¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
        created_plots = []

        # 1. éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿å¤–ã‚Œå€¤ã®å¯è¦–åŒ–
        if 'music_outliers' in analysis_results:
            plot_path = self._create_music_outliers_plot(analysis_results['music_outliers'])
            if plot_path:
                created_plots.append(plot_path)

        # 2. CV-LBæ ¼å·®è¦å› ã®å¯è¦–åŒ–
        if 'cv_lb_analysis' in analysis_results:
            plot_path = self._create_cv_lb_analysis_plot(analysis_results['cv_lb_analysis'])
            if plot_path:
                created_plots.append(plot_path)

        # 3. ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ãƒ»BPMå¸¯åˆ¥åˆ†å¸ƒã®å¯è¦–åŒ–
        if 'genre_bpm_distribution' in analysis_results:
            plot_path = self._create_genre_bpm_distribution_plot(analysis_results['genre_bpm_distribution'])
            if plot_path:
                created_plots.append(plot_path)

        # 4. æ¸¬å®šã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–
        if 'measurement_errors' in analysis_results:
            plot_path = self._create_measurement_errors_plot(analysis_results['measurement_errors'])
            if plot_path:
                created_plots.append(plot_path)

        logger.success(f"å¯è¦–åŒ–ä½œæˆå®Œäº†: {len(created_plots)}å€‹ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆ")
        return created_plots

    def _create_music_outliers_plot(self, outliers_data: Dict) -> Optional[Path]:
        """éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿å¤–ã‚Œå€¤ã®å¯è¦–åŒ–."""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # BPMåˆ†å¸ƒã¨extremeå€¤ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        bpm = self.data[self.target]
        axes[0, 0].hist(bpm, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].axvline(40, color='red', linestyle='--', label='Extreme Slow (<40)')
        axes[0, 0].axvline(200, color='red', linestyle='--', label='Extreme Fast (>200)')
        axes[0, 0].set_title(f'BPM Distribution\nExtreme values: {outliers_data["extreme_bpm"]["extreme_slow_count"] + outliers_data["extreme_bpm"]["extreme_fast_count"]} samples')
        axes[0, 0].set_xlabel('BeatsPerMinute')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].legend()

        # æ¥½å™¨ãƒãƒ©ãƒ³ã‚¹ç•°å¸¸ã®æ•£å¸ƒå›³
        x_feature = 'VocalContent'
        y_feature = 'InstrumentalScore'
        conflict_mask = (self.data[x_feature] > 0.8) & (self.data[y_feature] > 0.8)

        axes[0, 1].scatter(self.data[x_feature], self.data[y_feature], alpha=0.6, c='blue', s=10, label='Normal')
        axes[0, 1].scatter(self.data.loc[conflict_mask, x_feature], self.data.loc[conflict_mask, y_feature],
                          c='red', s=20, label=f'Conflict ({sum(conflict_mask)} samples)')
        axes[0, 1].set_xlabel(x_feature)
        axes[0, 1].set_ylabel(y_feature)
        axes[0, 1].set_title('Vocal-Instrumental Balance Conflicts')
        axes[0, 1].legend()

        # AudioLoudnessç•°å¸¸å€¤
        audio_data = self.data['AudioLoudness']
        normal_mask = (audio_data >= -30) & (audio_data <= 0)
        outlier_mask = ~normal_mask

        axes[1, 0].hist(audio_data[normal_mask], bins=30, alpha=0.7, color='green', label='Normal Range')
        axes[1, 0].hist(audio_data[outlier_mask], bins=30, alpha=0.7, color='red', label=f'Outliers ({sum(outlier_mask)} samples)')
        axes[1, 0].set_xlabel('AudioLoudness (dB)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('AudioLoudness Distribution')
        axes[1, 0].legend()

        # æ¥½æ›²é•·ç•°å¸¸
        duration_data = self.data['TrackDurationMs'] / 1000  # ç§’ã«å¤‰æ›
        normal_duration_mask = (duration_data >= 30) & (duration_data <= 600)

        axes[1, 1].hist(duration_data[normal_duration_mask], bins=30, alpha=0.7, color='blue', label='Normal Duration')
        axes[1, 1].hist(duration_data[~normal_duration_mask], bins=30, alpha=0.7, color='orange',
                       label=f'Abnormal ({sum(~normal_duration_mask)} samples)')
        axes[1, 1].set_xlabel('Track Duration (seconds)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Track Duration Distribution')
        axes[1, 1].legend()

        plt.tight_layout()
        plot_path = self.output_dir / "music_outliers_analysis.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        return plot_path

    def _create_cv_lb_analysis_plot(self, cv_lb_data: Dict) -> Optional[Path]:
        """CV-LBæ ¼å·®è¦å› ã®å¯è¦–åŒ–."""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # æ­ªåº¦åˆ†æ
        if 'skewness' in cv_lb_data['distribution_analysis']:
            skewness = cv_lb_data['distribution_analysis']['skewness']
            features = list(skewness.keys())
            skew_values = list(skewness.values())

            colors = ['red' if abs(val) > 1.0 else 'blue' for val in skew_values]
            axes[0, 0].bar(range(len(features)), skew_values, color=colors, alpha=0.7)
            axes[0, 0].axhline(1.0, color='red', linestyle='--', alpha=0.5, label='High Skew Threshold')
            axes[0, 0].axhline(-1.0, color='red', linestyle='--', alpha=0.5)
            axes[0, 0].set_xticks(range(len(features)))
            axes[0, 0].set_xticklabels(features, rotation=45, ha='right')
            axes[0, 0].set_title('Feature Skewness (CV-LB Gap Factor)')
            axes[0, 0].set_ylabel('Skewness')
            axes[0, 0].legend()

        # å°–åº¦åˆ†æ
        if 'kurtosis' in cv_lb_data['distribution_analysis']:
            kurtosis = cv_lb_data['distribution_analysis']['kurtosis']
            features = list(kurtosis.keys())
            kurt_values = list(kurtosis.values())

            colors = ['red' if abs(val) > 3.0 else 'green' for val in kurt_values]
            axes[0, 1].bar(range(len(features)), kurt_values, color=colors, alpha=0.7)
            axes[0, 1].axhline(3.0, color='red', linestyle='--', alpha=0.5, label='High Kurtosis Threshold')
            axes[0, 1].axhline(-3.0, color='red', linestyle='--', alpha=0.5)
            axes[0, 1].set_xticks(range(len(features)))
            axes[0, 1].set_xticklabels(features, rotation=45, ha='right')
            axes[0, 1].set_title('Feature Kurtosis (Distribution Shape)')
            axes[0, 1].set_ylabel('Kurtosis')
            axes[0, 1].legend()

        # éç·šå½¢é–¢ä¿‚ã‚¹ã‚³ã‚¢
        if 'nonlinear_relationships' in cv_lb_data:
            nonlinear = cv_lb_data['nonlinear_relationships']
            features = list(nonlinear.keys())
            nonlinear_scores = [data['nonlinearity_score'] for data in nonlinear.values()]

            colors = ['red' if score > 0.1 else 'blue' for score in nonlinear_scores]
            axes[1, 0].bar(range(len(features)), nonlinear_scores, color=colors, alpha=0.7)
            axes[1, 0].axhline(0.1, color='red', linestyle='--', alpha=0.5, label='High Nonlinearity Threshold')
            axes[1, 0].set_xticks(range(len(features)))
            axes[1, 0].set_xticklabels(features, rotation=45, ha='right')
            axes[1, 0].set_title('Nonlinearity Score (Spearman - Pearson)')
            axes[1, 0].set_ylabel('Nonlinearity Score')
            axes[1, 0].legend()

        # BPMå¯†åº¦åˆ†å¸ƒ
        if 'bpm_density_distribution' in cv_lb_data:
            density_dist = cv_lb_data['bpm_density_distribution']['distribution']
            ranges = list(density_dist.keys())
            densities = list(density_dist.values())

            axes[1, 1].bar(range(len(ranges)), densities, color='purple', alpha=0.7)
            axes[1, 1].set_xticks(range(len(ranges)))
            axes[1, 1].set_xticklabels(ranges, rotation=45, ha='right')
            axes[1, 1].set_title('BPM Range Density Distribution')
            axes[1, 1].set_ylabel('Density')

        plt.tight_layout()
        plot_path = self.output_dir / "cv_lb_gap_analysis.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        return plot_path

    def _create_genre_bpm_distribution_plot(self, genre_data: Dict) -> Optional[Path]:
        """ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ãƒ»BPMå¸¯åˆ¥åˆ†å¸ƒã®å¯è¦–åŒ–."""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # ã‚¸ãƒ£ãƒ³ãƒ«ä»£ç†å¤‰æ•°ã‚’ä½œæˆ
        genre_df = self._create_genre_proxies()
        genre_columns = ['Electronic_Proxy', 'Acoustic_Proxy', 'Vocal_Heavy', 'Dance_Proxy', 'Ambient_Proxy']

        # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥BPMç®±ã²ã’å›³
        genre_bpm_data = []
        genre_labels = []
        for genre in genre_columns:
            if genre in genre_df.columns:
                genre_mask = genre_df[genre] == 1
                if sum(genre_mask) > 10:
                    bpm_subset = self.data.loc[genre_mask, self.target]
                    genre_bpm_data.append(bpm_subset)
                    genre_labels.append(genre.replace('_Proxy', '').replace('_', ' '))

        if genre_bpm_data:
            axes[0, 0].boxplot(genre_bpm_data, labels=genre_labels)
            axes[0, 0].set_title('BPM Distribution by Genre Proxy')
            axes[0, 0].set_ylabel('BeatsPerMinute')
            axes[0, 0].tick_params(axis='x', rotation=45)

        # BPMç¯„å›²åˆ¥ã‚µãƒ³ãƒ—ãƒ«æ•°
        bpm_ranges = self._create_bpm_ranges()
        range_counts = bpm_ranges.value_counts()

        axes[0, 1].bar(range(len(range_counts)), range_counts.values, color='orange', alpha=0.7)
        axes[0, 1].set_xticks(range(len(range_counts)))
        axes[0, 1].set_xticklabels(range_counts.index, rotation=45, ha='right')
        axes[0, 1].set_title('Sample Count by BPM Range')
        axes[0, 1].set_ylabel('Count')

        # ã‚¸ãƒ£ãƒ³ãƒ«æ¤œå‡ºç‡
        if 'genre_counts' in genre_data:
            genre_counts = genre_data['genre_counts']
            valid_genres = {k: v for k, v in genre_counts.items() if v > 0}

            axes[1, 0].bar(range(len(valid_genres)), valid_genres.values(), color='green', alpha=0.7)
            axes[1, 0].set_xticks(range(len(valid_genres)))
            axes[1, 0].set_xticklabels([k.replace('_Proxy', '').replace('_', ' ') for k in valid_genres.keys()],
                                     rotation=45, ha='right')
            axes[1, 0].set_title('Genre Proxy Detection Count')
            axes[1, 0].set_ylabel('Detected Samples')

        # å…¨BPMåˆ†å¸ƒï¼ˆå‚è€ƒï¼‰
        axes[1, 1].hist(self.data[self.target], bins=30, color='lightblue', alpha=0.7, edgecolor='black')
        axes[1, 1].set_title('Overall BPM Distribution')
        axes[1, 1].set_xlabel('BeatsPerMinute')
        axes[1, 1].set_ylabel('Frequency')

        plt.tight_layout()
        plot_path = self.output_dir / "genre_bpm_distribution.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        return plot_path

    def _create_measurement_errors_plot(self, error_data: Dict) -> Optional[Path]:
        """æ¸¬å®šã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–."""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # ãƒ‰ãƒ¡ã‚¤ãƒ³é•åã®å¯è¦–åŒ–
        if 'domain_violations' in error_data:
            violations = error_data['domain_violations']
            features = []
            violation_counts = []

            for feature, data in violations.items():
                violation_count = data.get('out_of_range_total', data.get('suspicious_total', 0))
                features.append(feature)
                violation_counts.append(violation_count)

            colors = ['red' if count > 0 else 'green' for count in violation_counts]
            axes[0, 0].bar(range(len(features)), violation_counts, color=colors, alpha=0.7)
            axes[0, 0].set_xticks(range(len(features)))
            axes[0, 0].set_xticklabels(features, rotation=45, ha='right')
            axes[0, 0].set_title('Domain Violations by Feature')
            axes[0, 0].set_ylabel('Violation Count')

        # è«–ç†çš„æ•´åˆæ€§å•é¡Œ
        if 'logical_inconsistencies' in error_data:
            logical = error_data['logical_inconsistencies']
            check_names = []
            inconsistency_counts = []

            for check_name, data in logical.items():
                check_names.append(check_name.replace('_', ' ').title())
                inconsistency_counts.append(data.get('impossible_count', 0))

            axes[0, 1].bar(range(len(check_names)), inconsistency_counts, color='orange', alpha=0.7)
            axes[0, 1].set_xticks(range(len(check_names)))
            axes[0, 1].set_xticklabels(check_names, rotation=45, ha='right')
            axes[0, 1].set_title('Logical Inconsistency Issues')
            axes[0, 1].set_ylabel('Issue Count')

        # æ•°å€¤ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³
        if 'numerical_anomalies' in error_data:
            numerical = error_data['numerical_anomalies']
            features = list(numerical.keys())
            zero_ratios = [data['zero_ratio'] for data in numerical.values()]

            axes[1, 0].bar(range(len(features)), zero_ratios, color='purple', alpha=0.7)
            axes[1, 0].set_xticks(range(len(features)))
            axes[1, 0].set_xticklabels(features, rotation=45, ha='right')
            axes[1, 0].set_title('Zero Value Ratio by Feature')
            axes[1, 0].set_ylabel('Zero Ratio')

        # ç›¸é–¢ç•°å¸¸
        if 'correlation_anomalies' in error_data:
            high_corr_count = error_data['correlation_anomalies']['high_correlation_count']
            total_pairs = len(self.basic_features) * (len(self.basic_features) - 1) // 2

            axes[1, 1].pie([high_corr_count, total_pairs - high_corr_count],
                          labels=[f'High Correlation\n({high_corr_count})', f'Normal\n({total_pairs - high_corr_count})'],
                          colors=['red', 'green'], autopct='%1.1f%%')
            axes[1, 1].set_title('Feature Correlation Pattern\n(>0.95 threshold)')

        plt.tight_layout()
        plot_path = self.output_dir / "measurement_errors.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        return plot_path

    def generate_analysis_report(self, analysis_results: Dict) -> Path:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ.

        Args:
            analysis_results: å…¨åˆ†æçµæœã®è¾æ›¸

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        logger.info("åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")

        report_path = self.output_dir / "advanced_eda_report.md"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# é«˜åº¦éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿EDAåˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write(f"**åˆ†ææ—¥æ™‚**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: {self.data_path.name}\n")
            f.write(f"**ã‚µãƒ³ãƒ—ãƒ«æ•°**: {len(self.data):,}\n")
            f.write(f"**ç‰¹å¾´é‡æ•°**: {len(self.data.columns)}\n\n")

            f.write("## æˆ¦ç•¥è»¢æ›ã®èƒŒæ™¯\n\n")
            f.write("- **ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–é™ç•Œ**: TICKET-021ã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–å®Œäº†\n")
            f.write("- **CV-LBæ ¼å·®**: +0.076ã®ä¸€è²«ã—ãŸä¸‹æŒ¯ã‚Œ\n")
            f.write("- **æ ¹æœ¬åŸå› **: ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œã«ã‚ˆã‚‹æ±åŒ–æ€§èƒ½é™ç•Œ\n")
            f.write("- **æ–°æˆ¦ç•¥**: ãƒ‡ãƒ¼ã‚¿æ”¹å–„ã«ã‚ˆã‚‹CV-LBä¸€è²«æ€§å‘ä¸Š\n\n")

            # 1. éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿å¤–ã‚Œå€¤åˆ†æ
            if 'music_outliers' in analysis_results:
                f.write("## 1. éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®å¤–ã‚Œå€¤åˆ†æ\n\n")
                outliers = analysis_results['music_outliers']

                f.write("### ğŸµ æ¥µç«¯ãªBPMå€¤ï¼ˆéŸ³æ¥½ç†è«–é•åï¼‰\n")
                f.write(f"- **ç•°å¸¸ã«é…ã„æ¥½æ›²** (<40 BPM): {outliers['extreme_bpm']['extreme_slow_count']:,}ä»¶ ")
                f.write(f"({outliers['extreme_bpm']['extreme_slow_ratio']:.3%})\n")
                f.write(f"- **ç•°å¸¸ã«é€Ÿã„æ¥½æ›²** (>200 BPM): {outliers['extreme_bpm']['extreme_fast_count']:,}ä»¶ ")
                f.write(f"({outliers['extreme_bpm']['extreme_fast_ratio']:.3%})\n\n")

                f.write("### ğŸšï¸ æ¥½å™¨ãƒãƒ©ãƒ³ã‚¹ç•°å¸¸\n")
                balance = outliers['balance_anomalies']
                f.write(f"- **ãƒœãƒ¼ã‚«ãƒ«ãƒ»æ¥½å™¨çŸ›ç›¾**: {balance['vocal_instrumental_conflict_count']:,}ä»¶ ")
                f.write(f"({balance['vocal_instrumental_conflict_ratio']:.3%})\n")
                f.write(f"- **éŸ³éŸ¿ãƒ¬ãƒ™ãƒ«ç•°å¸¸**: {balance['audio_loudness_outliers_count']:,}ä»¶ ")
                f.write(f"({balance['audio_outlier_ratio']:.3%})\n\n")

                f.write("### â±ï¸ æ¥½æ›²é•·ç•°å¸¸\n")
                duration = outliers['duration_anomalies']
                f.write(f"- **ç•°å¸¸ã«çŸ­ã„** (<30ç§’): {duration['too_short_count']:,}ä»¶\n")
                f.write(f"- **ç•°å¸¸ã«é•·ã„** (>10åˆ†): {duration['too_long_count']:,}ä»¶\n")
                f.write(f"- **å¹³å‡é•·**: {duration['duration_stats']['mean_seconds']:.1f}ç§’\n\n")

            # 2. CV-LBæ ¼å·®è¦å› åˆ†æ
            if 'cv_lb_analysis' in analysis_results:
                f.write("## 2. CV-LBæ ¼å·®è¦å› åˆ†æ\n\n")
                cv_lb = analysis_results['cv_lb_analysis']

                f.write("### ğŸ“Š åˆ†å¸ƒã®æ­ªã¿ï¼ˆCV-LBä¸ä¸€è‡´è¦å› ï¼‰\n")
                if 'high_skew_features' in cv_lb:
                    high_skew = cv_lb['high_skew_features']
                    if high_skew:
                        f.write("**é«˜ã„æ­ªåº¦ã‚’æŒã¤ç‰¹å¾´é‡**:\n")
                        for feature, skew_val in sorted(high_skew.items(), key=lambda x: abs(x[1]), reverse=True):
                            f.write(f"- {feature}: {skew_val:.3f}\n")
                    else:
                        f.write("- é¡•è‘—ãªæ­ªã¿ã‚’æŒã¤ç‰¹å¾´é‡ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\n")
                    f.write("\n")

                f.write("### ğŸ”„ éç·šå½¢é–¢ä¿‚ï¼ˆäºˆæ¸¬å›°é›£æ€§ï¼‰\n")
                if 'high_nonlinear_features' in cv_lb:
                    high_nonlinear = cv_lb['high_nonlinear_features']
                    if high_nonlinear:
                        f.write("**éç·šå½¢æ€§ã®å¼·ã„ç‰¹å¾´é‡**:\n")
                        for feature, scores in sorted(high_nonlinear.items(),
                                                    key=lambda x: x[1]['nonlinearity_score'], reverse=True):
                            f.write(f"- {feature}: éç·šå½¢ã‚¹ã‚³ã‚¢ {scores['nonlinearity_score']:.3f}\n")
                    else:
                        f.write("- é¡•è‘—ãªéç·šå½¢é–¢ä¿‚ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\n")
                    f.write("\n")

                f.write("### ğŸ“ˆ BPMç¯„å›²åˆ¥ãƒ‡ãƒ¼ã‚¿å¯†åº¦\n")
                if 'bpm_density_distribution' in cv_lb:
                    density = cv_lb['bpm_density_distribution']
                    f.write(f"- **æœ€ã‚‚å¤šã„BPMå¸¯**: {density['most_common_range']}\n")
                    f.write(f"- **æœ€ã‚‚å°‘ãªã„BPMå¸¯**: {density['least_common_range']}\n")
                    f.write(f"- **å¯†åº¦åˆ†æ•£**: {density['density_variance']:.4f}\n\n")

            # 3. æ¸¬å®šèª¤å·®åˆ†æ
            if 'measurement_errors' in analysis_results:
                f.write("## 3. æ¸¬å®šèª¤å·®ãƒ»è¨˜éŒ²ã‚¨ãƒ©ãƒ¼åˆ†æ\n\n")
                errors = analysis_results['measurement_errors']

                f.write("### âš ï¸ ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜é•å\n")
                if 'domain_violations' in errors:
                    violations = errors['domain_violations']
                    total_violations = 0
                    for feature, violation_data in violations.items():
                        violation_count = violation_data.get('out_of_range_total',
                                                           violation_data.get('suspicious_total', 0))
                        if violation_count > 0:
                            ratio = violation_data.get('out_of_range_ratio',
                                                     violation_data.get('suspicious_ratio', 0))
                            f.write(f"- **{feature}**: {violation_count:,}ä»¶ã®ç¯„å›²å¤–å€¤ ({ratio:.3%})\n")
                            total_violations += violation_count

                    if total_violations == 0:
                        f.write("- é‡å¤§ãªãƒ‰ãƒ¡ã‚¤ãƒ³é•åã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\n")
                    f.write("\n")

                f.write("### ğŸ” è«–ç†çš„æ•´åˆæ€§å•é¡Œ\n")
                if 'logical_inconsistencies' in errors:
                    logical = errors['logical_inconsistencies']
                    for check_name, check_data in logical.items():
                        if check_data.get('impossible_count', 0) > 0:
                            f.write(f"- **{check_name}**: {check_data['impossible_count']:,}ä»¶ ")
                            f.write(f"({check_data['impossible_ratio']:.3%})\n")
                    f.write("\n")

            # 4. æ¨å¥¨æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            f.write("## 4. æ¨å¥¨ãƒ‡ãƒ¼ã‚¿æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n\n")

            f.write("### ğŸ† **å„ªå…ˆåº¦: æœ€é«˜** - CV-LBæ ¼å·®æ”¹å–„ã¸ã®ç›´æ¥çš„åŠ¹æœ\n")
            f.write("1. **æ¥µç«¯ãªBPMå€¤ã®é™¤å»**\n")
            f.write("   - 40 BPMæœªæº€ã€200 BPMè¶…éã®ã‚µãƒ³ãƒ—ãƒ«é™¤å»\n")
            f.write("   - éŸ³æ¥½ç†è«–çš„ã«ä¸é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ã®å“è³ªå‘ä¸Š\n\n")

            f.write("2. **ç¯„å›²å¤–å€¤ã®ä¿®æ­£**\n")
            f.write("   - ç¢ºç‡ç‰¹å¾´é‡ã®è² å€¤ãƒ»1è¶…éå€¤ã®ä¿®æ­£\n")
            f.write("   - AudioLoudnessã®ç‰©ç†çš„é™ç•Œå¤–å€¤ã®å‡¦ç†\n\n")

            f.write("### ğŸ¯ **å„ªå…ˆåº¦: é«˜** - äºˆæ¸¬ç²¾åº¦å‘ä¸Šã¸ã®åŠ¹æœ\n")
            f.write("1. **é«˜ã„æ­ªåº¦ç‰¹å¾´é‡ã®å¤‰æ›**\n")
            f.write("   - å¯¾æ•°å¤‰æ›ã€Box-Coxå¤‰æ›ã®é©ç”¨\n")
            f.write("   - åˆ†å¸ƒæ­£è¦åŒ–ã«ã‚ˆã‚‹æ±åŒ–æ€§èƒ½æ”¹å–„\n\n")

            f.write("2. **éç·šå½¢é–¢ä¿‚ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**\n")
            f.write("   - å¤šé …å¼ç‰¹å¾´é‡ã€äº¤äº’ä½œç”¨é …ã®è¿½åŠ \n")
            f.write("   - ãƒ¢ãƒ‡ãƒ«ã®éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³æ•æ‰èƒ½åŠ›å‘ä¸Š\n\n")

            f.write("### ğŸ”§ **å„ªå…ˆåº¦: ä¸­** - ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š\n")
            f.write("1. **è«–ç†çš„çŸ›ç›¾ã®è§£æ±º**\n")
            f.write("   - ãƒœãƒ¼ã‚«ãƒ«+æ¥½å™¨ã‚¹ã‚³ã‚¢åˆè¨ˆ>1ã®èª¿æ•´\n")
            f.write("   - ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»ãƒ ãƒ¼ãƒ‰ä¸æ•´åˆã®ä¿®æ­£\n\n")

            f.write("2. **ç•°åˆ†æ•£æ€§ã®æ”¹å–„**\n")
            f.write("   - BPMå¸¯åŸŸåˆ¥ã®ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n")
            f.write("   - æ¡ä»¶ä»˜ãæ­£è¦åŒ–ã®é©ç”¨\n\n")

            # 5. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
            f.write("## 5. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆPhase 2-4ï¼‰\n\n")
            f.write("### Phase 2: å¤–ã‚Œå€¤æ¤œå‡ºãƒ»é™¤å»ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…\n")
            f.write("- `src/data/outlier_handler.py` ã®ä½œæˆ\n")
            f.write("- çµ±è¨ˆçš„æ‰‹æ³•ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®çµ„ã¿åˆã‚ã›\n")
            f.write("- æ®µéšçš„é™¤å»ã«ã‚ˆã‚‹æ€§èƒ½æ”¹å–„åŠ¹æœã®æ¤œè¨¼\n\n")

            f.write("### Phase 3: éŸ³æ¥½ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ã®å®Ÿè£…\n")
            f.write("- `src/features/music_theory.py` ã®ä½œæˆ\n")
            f.write("- ãƒãƒ¼ãƒ¢ãƒ‹ãƒ¼è§£æã€ãƒªã‚ºãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡\n")
            f.write("- æ¥½æ›²æ§‹é€ æ¨å®šã«ã‚ˆã‚‹å·®åˆ¥åŒ–\n\n")

            f.write("### Phase 4: ãƒ‡ãƒ¼ã‚¿å“è³ªæœ€é©åŒ–\n")
            f.write("- æ¬ æå€¤ã®éŸ³æ¥½ç†è«–ãƒ™ãƒ¼ã‚¹è£œå®Œ\n")
            f.write("- æ¸¬å®šèª¤å·®ãƒ»è¨˜éŒ²ã‚¨ãƒ©ãƒ¼å¯¾ç­–\n")
            f.write("- ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£\n\n")

            f.write("---\n")
            f.write(f"**åˆ†æå®Œäº†**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("**æœŸå¾…åŠ¹æœ**: CV-LBæ ¼å·® +0.076 â†’ +0.050ä»¥ä¸‹, LBæ€§èƒ½ 26.385æœªæº€é”æˆ\n")

        logger.success(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
        return report_path

    def run_comprehensive_analysis(self) -> Dict:
        """åŒ…æ‹¬çš„ãªåˆ†æã‚’å®Ÿè¡Œ.

        Returns:
            å…¨åˆ†æçµæœã®è¾æ›¸
        """
        logger.info("ğŸµ é«˜åº¦éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿EDAåˆ†æã‚’é–‹å§‹...")
        logger.info("æˆ¦ç•¥: ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šã«ã‚ˆã‚‹CV-LBä¸€è²«æ€§æ”¹å–„")

        if not self.load_data():
            return {}

        results = {}

        try:
            # å„åˆ†æã‚’é †æ¬¡å®Ÿè¡Œ
            results['music_outliers'] = self.analyze_music_outliers()
            results['cv_lb_analysis'] = self.analyze_cv_lb_gap_factors()
            results['genre_bpm_distribution'] = self.analyze_bpm_genre_distribution()
            results['measurement_errors'] = self.detect_measurement_errors()

            # å¯è¦–åŒ–ä½œæˆ
            plot_paths = self.create_comprehensive_visualizations(results)
            results['visualizations'] = [str(path) for path in plot_paths]

            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report_path = self.generate_analysis_report(results)
            results['report_path'] = str(report_path)

            # åˆ†æçµæœã‚’ä¿å­˜
            self.analysis_results = results

            logger.success("ğŸ† åŒ…æ‹¬çš„ãªé«˜åº¦EDAåˆ†æå®Œäº†")
            return results

        except Exception as e:
            logger.error(f"åˆ†æå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return {}


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°."""
    logger.info("TICKET-024: é«˜åº¦éŸ³æ¥½ãƒ‡ãƒ¼ã‚¿EDAãƒ»å•é¡Œç‰¹å®šã‚·ã‚¹ãƒ†ãƒ ")

    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹è¨­å®šï¼ˆçµ±ä¸€ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨ï¼‰
    train_data_path = PROCESSED_DATA_DIR / "train_unified_75_features.csv"

    if not train_data_path.exists():
        logger.error(f"çµ±ä¸€ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {train_data_path}")
        logger.info("ä»£æ›¿æ‰‹æ®µ: åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿè¡Œã‚’è©¦è¡Œ")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        basic_train_path = PROCESSED_DATA_DIR / "train.csv"
        if basic_train_path.exists():
            train_data_path = basic_train_path
            logger.info(f"åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿè¡Œ: {train_data_path}")
        else:
            logger.error("åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            logger.info("å…ˆã«ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„: python src/dataset.py")
            return

    # é«˜åº¦EDAå®Ÿè¡Œ
    eda = AdvancedMusicEDA(train_data_path)
    results = eda.run_comprehensive_analysis()

    if results:
        logger.success("âœ… é«˜åº¦EDAåˆ†æãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")

        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        if 'report_path' in results:
            logger.info(f"ğŸ“Š åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {results['report_path']}")

        if 'visualizations' in results:
            logger.info(f"ğŸ“ˆ å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(results['visualizations'])}")

        # é‡è¦ãªç™ºè¦‹ã‚’è¡¨ç¤º
        if 'music_outliers' in results:
            outliers = results['music_outliers']
            extreme_total = (outliers['extreme_bpm']['extreme_slow_count'] +
                           outliers['extreme_bpm']['extreme_fast_count'])
            if extreme_total > 0:
                logger.warning(f"ğŸš¨ ç™ºè¦‹: æ¥µç«¯ãªBPMå€¤ {extreme_total}ä»¶ (è¦å¯¾å‡¦)")

        if 'measurement_errors' in results:
            errors = results['measurement_errors']
            if 'domain_violations' in errors:
                total_violations = sum(
                    data.get('out_of_range_total', data.get('suspicious_total', 0))
                    for data in errors['domain_violations'].values()
                )
                if total_violations > 0:
                    logger.warning(f"âš ï¸ ç™ºè¦‹: ãƒ‰ãƒ¡ã‚¤ãƒ³é•å {total_violations}ä»¶ (è¦ä¿®æ­£)")

        logger.info("ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Phase 2å¤–ã‚Œå€¤é™¤å»ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…")

    else:
        logger.error("âŒ é«˜åº¦EDAåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()