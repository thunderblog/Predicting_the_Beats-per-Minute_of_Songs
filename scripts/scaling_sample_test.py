#!/usr/bin/env python3
"""
ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºæ®µéšçš„æ‹¡å¤§ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç›®çš„:
- åŒ…æ‹¬çš„äº¤äº’ä½œç”¨ç‰¹å¾´é‡ãŒåŠ¹æœã‚’ç™ºæ®ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’ç‰¹å®š
- 500, 1000, 2000, 5000ä»¶ã§ã®æ€§èƒ½æ¯”è¼ƒ
- ç‰¹å¾´é‡æ•°ã¨ã‚µãƒ³ãƒ—ãƒ«æ•°ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ã‚’è§£æ˜
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import warnings
import time
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
proj_root = Path(__file__).parent.parent
sys.path.append(str(proj_root))

from src.features import create_comprehensive_interaction_features
from src.config import PROCESSED_DATA_DIR

def load_scaled_sample(sample_size: int):
    """æŒ‡å®šã‚µã‚¤ã‚ºã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­ï¼ˆã‚µã‚¤ã‚º: {sample_size}ï¼‰...")

    train_df = pd.read_csv(PROCESSED_DATA_DIR / "train.csv")

    if len(train_df) > sample_size:
        sample_df = train_df.sample(n=sample_size, random_state=42)
    else:
        sample_df = train_df
        logger.warning(f"è¦æ±‚ã‚µã‚¤ã‚º{sample_size}ä»¶ã«å¯¾ã—ã¦å®Ÿéš›ã¯{len(sample_df)}ä»¶")

    logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(sample_df)}ä»¶")
    return sample_df

def evaluate_with_size(X, y, feature_name="Default", sample_size=0):
    """ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸé©åˆ‡ãªè¨­å®šã§ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""

    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´
    if sample_size <= 1000:
        cv_folds = 3
        n_estimators = 50
        num_leaves = 31
    elif sample_size <= 3000:
        cv_folds = 4
        n_estimators = 100
        num_leaves = 31
    else:
        cv_folds = 5
        n_estimators = 150
        num_leaves = 63

    model_params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': num_leaves,
        'learning_rate': 0.1,
        'n_estimators': n_estimators,
        'verbose': -1,
        'random_state': 42
    }

    logger.info(f"  ãƒ¢ãƒ‡ãƒ«è¨­å®š: {cv_folds}-fold CV, n_estimators={n_estimators}, num_leaves={num_leaves}")

    # é©å¿œçš„CV
    kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
    cv_scores = []

    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):
        X_train_fold = X.iloc[train_idx]
        X_val_fold = X.iloc[val_idx]
        y_train_fold = y.iloc[train_idx]
        y_val_fold = y.iloc[val_idx]

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = lgb.LGBMRegressor(**model_params)
        model.fit(X_train_fold, y_train_fold)

        # äºˆæ¸¬ãƒ»è©•ä¾¡
        y_pred = model.predict(X_val_fold)
        rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))
        cv_scores.append(rmse)

        logger.info(f"    Fold {fold+1}: RMSE = {rmse:.4f}")

    avg_rmse = np.mean(cv_scores)
    std_rmse = np.std(cv_scores)

    return {
        'feature_type': feature_name,
        'sample_size': sample_size,
        'n_features': X.shape[1],
        'avg_rmse': avg_rmse,
        'std_rmse': std_rmse,
        'cv_scores': cv_scores,
        'cv_folds': cv_folds,
        'n_estimators': n_estimators
    }

def test_sample_size(sample_size: int):
    """æŒ‡å®šã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³vsåŒ…æ‹¬çš„äº¤äº’ä½œç”¨ã‚’æ¯”è¼ƒ"""
    logger.info(f"\n=== ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º {sample_size}ä»¶ã§ã®ãƒ†ã‚¹ãƒˆ ===")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    sample_data = load_scaled_sample(sample_size)
    y = sample_data['BeatsPerMinute']

    # åŸºæœ¬ç‰¹å¾´é‡
    basic_features = [
        'RhythmScore', 'AudioLoudness', 'VocalContent', 'AcousticQuality',
        'InstrumentalScore', 'LivePerformanceLikelihood', 'MoodScore',
        'TrackDurationMs', 'Energy'
    ]

    results = []

    # 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡
    logger.info("1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆåŸºæœ¬ç‰¹å¾´é‡ï¼‰ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
    start_time = time.time()
    X_baseline = sample_data[basic_features]
    result_baseline = evaluate_with_size(X_baseline, y, "Baseline", sample_size)
    result_baseline['processing_time'] = time.time() - start_time
    results.append(result_baseline)

    # 2. åŒ…æ‹¬çš„äº¤äº’ä½œç”¨ç‰¹å¾´é‡è©•ä¾¡
    logger.info("2. åŒ…æ‹¬çš„äº¤äº’ä½œç”¨ç‰¹å¾´é‡ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
    start_time = time.time()
    enhanced_data = create_comprehensive_interaction_features(sample_data)
    feature_cols = [col for col in enhanced_data.columns if col not in ['id', 'BeatsPerMinute']]
    X_enhanced = enhanced_data[feature_cols]
    result_enhanced = evaluate_with_size(X_enhanced, y, "Enhanced", sample_size)
    result_enhanced['processing_time'] = time.time() - start_time
    results.append(result_enhanced)

    # çµæœæ¯”è¼ƒ
    improvement = result_baseline['avg_rmse'] - result_enhanced['avg_rmse']
    improvement_pct = (improvement / result_baseline['avg_rmse']) * 100

    logger.info(f"\nğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º {sample_size}ä»¶ã®çµæœ:")
    logger.info(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {result_baseline['avg_rmse']:.4f} (Â±{result_baseline['std_rmse']:.4f})")
    logger.info(f"  æ‹¡å¼µç‰¹å¾´é‡:   {result_enhanced['avg_rmse']:.4f} (Â±{result_enhanced['std_rmse']:.4f})")
    logger.info(f"  æ”¹å–„:         {improvement:+.4f} ({improvement_pct:+.2f}%)")
    logger.info(f"  ç‰¹å¾´é‡æ•°:     {result_baseline['n_features']} â†’ {result_enhanced['n_features']}")

    return {
        'sample_size': sample_size,
        'baseline': result_baseline,
        'enhanced': result_enhanced,
        'improvement': improvement,
        'improvement_pct': improvement_pct
    }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° - æ®µéšçš„ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆ"""
    logger.info("=== æ®µéšçš„ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆé–‹å§‹ ===")

    # ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
    sample_sizes = [500, 1000, 2000, 5000]
    all_results = []

    # å„ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§ãƒ†ã‚¹ãƒˆ
    for sample_size in sample_sizes:
        try:
            result = test_sample_size(sample_size)
            all_results.append(result)
        except Exception as e:
            logger.error(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º {sample_size} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    # çµæœã‚µãƒãƒªãƒ¼
    logger.info("\n=== æ®µéšçš„ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ ===")
    logger.info(f"{'ã‚µãƒ³ãƒ—ãƒ«':<8} {'ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³':<12} {'æ‹¡å¼µç‰¹å¾´é‡':<12} {'æ”¹å–„':<10} {'æ”¹å–„ç‡':<8} {'ç‰¹å¾´é‡æ•°':<10}")
    logger.info("-" * 70)

    best_sample_size = None
    best_improvement = -float('inf')

    for result in all_results:
        sample_size = result['sample_size']
        baseline_rmse = result['baseline']['avg_rmse']
        enhanced_rmse = result['enhanced']['avg_rmse']
        improvement = result['improvement']
        improvement_pct = result['improvement_pct']
        n_features = result['enhanced']['n_features']

        logger.info(f"{sample_size:<8} {baseline_rmse:<12.4f} {enhanced_rmse:<12.4f} "
                   f"{improvement:<10.4f} {improvement_pct:<8.2f}% {n_features:<10}")

        if improvement > best_improvement:
            best_improvement = improvement
            best_sample_size = sample_size

    logger.info("-" * 70)

    if best_sample_size:
        logger.success(f"ğŸ† æœ€è‰¯ã®çµæœ: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º {best_sample_size}ä»¶ (æ”¹å–„: {best_improvement:+.4f})")

        # æ¨å¥¨äº‹é …
        if best_improvement > 0:
            logger.success("âœ… åŒ…æ‹¬çš„äº¤äº’ä½œç”¨ç‰¹å¾´é‡ãŒæœ‰åŠ¹ã§ã™ï¼")
            logger.info(f"ğŸ“ æ¨å¥¨ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {best_sample_size}ä»¶ä»¥ä¸Š")
        else:
            logger.warning("âš ï¸ ã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    else:
        logger.error("âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")

    # è©³ç´°çµæœä¿å­˜
    import json
    results_file = Path("results/scaling_sample_test.json")
    results_file.parent.mkdir(exist_ok=True)

    summary = {
        'test_summary': {
            'sample_sizes_tested': sample_sizes,
            'best_sample_size': best_sample_size,
            'best_improvement': best_improvement
        },
        'detailed_results': all_results
    }

    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    logger.success(f"è©³ç´°çµæœã‚’ä¿å­˜: {results_file}")

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    if len(all_results) >= 2:
        logger.info("\nğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ:")
        for i in range(1, len(all_results)):
            prev = all_results[i-1]
            curr = all_results[i]

            trend = curr['improvement_pct'] - prev['improvement_pct']
            logger.info(f"  {prev['sample_size']} â†’ {curr['sample_size']}ä»¶: æ”¹å–„ç‡å¤‰åŒ– {trend:+.2f}%ãƒã‚¤ãƒ³ãƒˆ")

    return all_results

if __name__ == "__main__":
    main()