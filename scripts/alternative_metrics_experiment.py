"""
ä»£æ›¿è©•ä¾¡æŒ‡æ¨™å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

RMSEä»¥å¤–ã®è©•ä¾¡æŒ‡æ¨™ã§è¨“ç·´ã—ã€æœ€çµ‚çš„ãªRMSEæ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹å®Ÿé¨“
éŸ³æ¥½BPMäºˆæ¸¬ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹è©•ä¾¡æŒ‡æ¨™æœ€é©åŒ–ã®åŠ¹æœã‚’æ¤œè¨¼
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import json
import numpy as np
import pandas as pd
from datetime import datetime

# loguruè¨­å®šã‚’å®‰å…¨ã«åˆæœŸåŒ–
from loguru import logger
logger.remove()  # å…¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤
logger.add(sys.stderr, level="INFO")  # æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã«è¨­å®š
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from typing import Dict, List, Tuple, Any
import lightgbm as lgb
import catboost as cb

# è¨­å®šã‚’ç›´æ¥å®šç¾©ï¼ˆconfig.pyã®loguruå•é¡Œã‚’å›é¿ï¼‰
class Config:
    target = "BeatsPerMinute"
    num_leaves = 31
    learning_rate = 0.1
    feature_fraction = 0.8
    n_estimators = 1000
    stopping_rounds = 100
    random_state = 42

config = Config()

# CVæˆ¦ç•¥ã‚’ç›´æ¥å®Ÿè£…
from sklearn.model_selection import StratifiedKFold

def create_bpm_stratified_cv(n_splits=5, random_state=42):
    """BPM Stratified KFoldä½œæˆ"""
    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)


def huber_loss_sklearn(y_true: np.ndarray, y_pred: np.ndarray, delta: float = 1.0) -> float:
    """Huber Lossè¨ˆç®—ï¼ˆscikit-learné¢¨ï¼‰"""
    residual = np.abs(y_true - y_pred)
    return np.mean(np.where(residual <= delta,
                           0.5 * residual**2,
                           delta * (residual - 0.5 * delta)))

def quantile_loss(y_true: np.ndarray, y_pred: np.ndarray, alpha: float = 0.5) -> float:
    """Quantile Lossè¨ˆç®— (Pinball Loss)"""
    residual = y_true - y_pred
    return np.mean(np.maximum(alpha * residual, (alpha - 1) * residual))


class AlternativeMetricsExperiment:
    """ä»£æ›¿è©•ä¾¡æŒ‡æ¨™å®Ÿé¨“ã‚¯ãƒ©ã‚¹"""

    def __init__(self, data_path: str = None):
        self.data_path = data_path or "data/processed/train_unified_75_features.csv"
        self.results = {}

        # å®Ÿé¨“ã™ã‚‹è©•ä¾¡æŒ‡æ¨™ã®å®šç¾©
        self.metrics_config = {
            "rmse": {
                "lgb_metric": "rmse",
                "catboost_metric": "RMSE",
                "eval_func": lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),
                "description": "Root Mean Squared Error (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³)"
            },
            "mae": {
                "lgb_metric": "mae",
                "catboost_metric": "MAE",
                "eval_func": mean_absolute_error,
                "description": "Mean Absolute Error (å¤–ã‚Œå€¤è€æ€§)"
            },
            "mape": {
                "lgb_metric": "mape",
                "catboost_metric": "MAPE",
                "eval_func": mean_absolute_percentage_error,
                "description": "Mean Absolute Percentage Error (ç›¸å¯¾èª¤å·®)"
            },
            "huber": {
                "lgb_metric": "huber",
                "catboost_metric": "Huber",
                "eval_func": lambda y_true, y_pred: huber_loss_sklearn(y_true, y_pred, delta=1.0),
                "description": "Huber Loss (RMSE+MAEãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰)"
            },
            "quantile_50": {
                "lgb_metric": "quantile",
                "catboost_metric": "Quantile",
                "eval_func": lambda y_true, y_pred: quantile_loss(y_true, y_pred, alpha=0.5),
                "description": "Quantile Loss Î±=0.5 (ä¸­å¤®å€¤å›å¸°)",
                "lgb_params": {"alpha": 0.5},
                "catboost_params": {"delta": 0.5}
            }
        }

    def load_data(self) -> Tuple[pd.DataFrame, pd.Series]:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {self.data_path}")
        df = pd.read_csv(self.data_path)

        feature_cols = [col for col in df.columns if col not in ["id", config.target]]
        X = df[feature_cols]
        y = df[config.target]

        logger.info(f"ç‰¹å¾´é‡æ•°: {len(feature_cols)}, ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}")
        return X, y

    def train_lightgbm_with_metric(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        metric_name: str
    ) -> lgb.Booster:
        """æŒ‡å®šè©•ä¾¡æŒ‡æ¨™ã§LightGBMã‚’è¨“ç·´"""
        metric_config = self.metrics_config[metric_name]

        # ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params = {
            "objective": "regression",
            "metric": metric_config["lgb_metric"],
            "boosting_type": "gbdt",
            "num_leaves": config.num_leaves,
            "learning_rate": config.learning_rate,
            "feature_fraction": config.feature_fraction,
            "bagging_fraction": 0.8,
            "bagging_freq": 5,
            "verbose": -1,
            "random_state": config.random_state,
        }

        # ãƒ¡ãƒˆãƒªãƒƒã‚¯å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¿½åŠ 
        if "lgb_params" in metric_config:
            params.update(metric_config["lgb_params"])

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

        # è¨“ç·´å®Ÿè¡Œ
        model = lgb.train(
            params,
            train_data,
            valid_sets=[train_data, val_data],
            valid_names=["train", "eval"],
            num_boost_round=config.n_estimators,
            callbacks=[
                lgb.early_stopping(config.stopping_rounds),
                lgb.log_evaluation(0)  # ãƒ­ã‚°ç„¡åŠ¹åŒ–
            ],
        )

        return model

    def train_catboost_with_metric(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        metric_name: str
    ) -> cb.CatBoostRegressor:
        """æŒ‡å®šè©•ä¾¡æŒ‡æ¨™ã§CatBoostã‚’è¨“ç·´"""
        metric_config = self.metrics_config[metric_name]

        # ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params = {
            "loss_function": metric_config["catboost_metric"],
            "eval_metric": metric_config["catboost_metric"],
            "depth": 6,
            "learning_rate": config.learning_rate,
            "subsample": 0.8,
            "rsm": config.feature_fraction,
            "random_seed": config.random_state,
            "verbose": 0,
            "early_stopping_rounds": config.stopping_rounds,
            "iterations": config.n_estimators,
        }

        # ãƒ¡ãƒˆãƒªãƒƒã‚¯å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¿½åŠ 
        if "catboost_params" in metric_config:
            params.update(metric_config["catboost_params"])

        # ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»è¨“ç·´
        model = cb.CatBoostRegressor(**params)
        model.fit(
            X_train, y_train,
            eval_set=(X_val, y_val),
            verbose=False,
            plot=False
        )

        return model

    def evaluate_all_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """å…¨è©•ä¾¡æŒ‡æ¨™ã§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        results = {}
        for metric_name, metric_config in self.metrics_config.items():
            try:
                score = metric_config["eval_func"](y_true, y_pred)
                results[metric_name] = score
            except Exception as e:
                logger.warning(f"ãƒ¡ãƒˆãƒªãƒƒã‚¯ {metric_name} ã®è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                results[metric_name] = np.nan
        return results

    def run_cv_experiment(
        self,
        model_type: str = "lightgbm",
        metric_name: str = "rmse",
        n_folds: int = 5
    ) -> Dict[str, Any]:
        """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“å®Ÿè¡Œ"""
        logger.info(f"å®Ÿé¨“é–‹å§‹: {model_type} + {metric_name}")

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        X, y = self.load_data()

        # CVæˆ¦ç•¥ä½œæˆï¼ˆBPMå€¤ã§å±¤åŒ–åˆ†å‰²ï¼‰
        # BPMå€¤ã‚’ãƒ“ãƒ³ã«åˆ†ã‘ã¦å±¤åŒ–
        y_binned = pd.cut(y, bins=10, labels=False)
        cv_splitter = create_bpm_stratified_cv(n_splits=n_folds, random_state=config.random_state)

        fold_results = []
        models = []

        for fold, (train_idx, val_idx) in enumerate(cv_splitter.split(X, y_binned)):
            logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold + 1}/{n_folds}")

            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            if model_type == "lightgbm":
                model = self.train_lightgbm_with_metric(X_train, y_train, X_val, y_val, metric_name)
                y_pred = model.predict(X_val, num_iteration=model.best_iteration)
            elif model_type == "catboost":
                model = self.train_catboost_with_metric(X_train, y_train, X_val, y_val, metric_name)
                y_pred = model.predict(X_val)
            else:
                raise ValueError(f"Unsupported model_type: {model_type}")

            # å…¨ãƒ¡ãƒˆãƒªãƒƒã‚¯ã§è©•ä¾¡
            fold_scores = self.evaluate_all_metrics(y_val.values, y_pred)
            fold_results.append(fold_scores)
            models.append(model)

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰çµæœãƒ­ã‚°
            logger.info(f"  RMSE: {fold_scores['rmse']:.4f}, è¨“ç·´ãƒ¡ãƒˆãƒªãƒƒã‚¯({metric_name}): {fold_scores[metric_name]:.4f}")

        # çµæœé›†ç´„
        aggregated_results = {}
        for metric in self.metrics_config.keys():
            scores = [fold[metric] for fold in fold_results if not np.isnan(fold[metric])]
            if scores:
                aggregated_results[f"{metric}_mean"] = np.mean(scores)
                aggregated_results[f"{metric}_std"] = np.std(scores)

        experiment_result = {
            "model_type": model_type,
            "training_metric": metric_name,
            "n_folds": n_folds,
            "fold_results": fold_results,
            "aggregated_results": aggregated_results,
            "models": models,
            "timestamp": datetime.now().isoformat()
        }

        logger.success(f"å®Ÿé¨“å®Œäº†: RMSE {aggregated_results['rmse_mean']:.4f}Â±{aggregated_results['rmse_std']:.4f}")
        return experiment_result

    def run_full_experiment(self) -> Dict[str, Any]:
        """å…¨ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ»å…¨ãƒ¢ãƒ‡ãƒ«ã§ã®å®Œå…¨å®Ÿé¨“"""
        logger.info("ä»£æ›¿è©•ä¾¡æŒ‡æ¨™å®Ÿé¨“é–‹å§‹")

        all_results = {}

        # å®Ÿé¨“å¯¾è±¡ã®çµ„ã¿åˆã‚ã›
        model_types = ["lightgbm", "catboost"]
        metric_names = ["rmse", "mae", "huber", "mape"]  # quantile_50ã¯å¾Œå›ã—

        for model_type in model_types:
            all_results[model_type] = {}

            for metric_name in metric_names:
                try:
                    result = self.run_cv_experiment(model_type, metric_name)
                    all_results[model_type][metric_name] = result
                except Exception as e:
                    logger.error(f"å®Ÿé¨“å¤±æ•— {model_type}+{metric_name}: {e}")
                    all_results[model_type][metric_name] = {"error": str(e)}

        # çµæœä¿å­˜
        self.save_results(all_results)

        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
        self.generate_summary_report(all_results)

        return all_results

    def save_results(self, results: Dict[str, Any]):
        """å®Ÿé¨“çµæœä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_path = f"experiments/alternative_metrics_experiment_{timestamp}.json"

        # modelsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ä¿å­˜ã§ããªã„ã®ã§é™¤å¤–
        saveable_results = {}
        for model_type, model_results in results.items():
            saveable_results[model_type] = {}
            for metric, result in model_results.items():
                if isinstance(result, dict) and "models" in result:
                    # modelsã‚’é™¤å¤–ã—ã¦ã‚³ãƒ”ãƒ¼
                    saveable_result = {k: v for k, v in result.items() if k != "models"}
                    saveable_results[model_type][metric] = saveable_result
                else:
                    saveable_results[model_type][metric] = result

        Path("experiments").mkdir(exist_ok=True)
        with open(results_path, "w", encoding="utf-8") as f:
            json.dump(saveable_results, f, indent=2, ensure_ascii=False)

        logger.success(f"å®Ÿé¨“çµæœä¿å­˜: {results_path}")

    def generate_summary_report(self, results: Dict[str, Any]):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        logger.info("\n" + "="*60)
        logger.info("ä»£æ›¿è©•ä¾¡æŒ‡æ¨™å®Ÿé¨“ - ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ")
        logger.info("="*60)

        # çµæœæ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
        comparison_data = []

        for model_type, model_results in results.items():
            for metric_name, result in model_results.items():
                if isinstance(result, dict) and "aggregated_results" in result:
                    agg = result["aggregated_results"]
                    comparison_data.append({
                        "Model": model_type.upper(),
                        "Training_Metric": metric_name.upper(),
                        "CV_RMSE": f"{agg['rmse_mean']:.4f}Â±{agg['rmse_std']:.4f}",
                        "CV_MAE": f"{agg['mae_mean']:.4f}Â±{agg['mae_std']:.4f}" if 'mae_mean' in agg else "N/A",
                        "Training_Score": f"{agg[f'{metric_name}_mean']:.4f}Â±{agg[f'{metric_name}_std']:.4f}"
                    })

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
        if comparison_data:
            df_comparison = pd.DataFrame(comparison_data)
            print("\nğŸ“Š æ€§èƒ½æ¯”è¼ƒè¡¨:")
            print(df_comparison.to_string(index=False))

            # ãƒ™ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹å®š
            rmse_scores = []
            for data in comparison_data:
                rmse_str = data["CV_RMSE"].split("Â±")[0]
                rmse_scores.append((float(rmse_str), data["Model"], data["Training_Metric"]))

            best_rmse = min(rmse_scores)
            logger.info(f"\nğŸ† æœ€é«˜RMSEæ€§èƒ½: {best_rmse[0]:.4f} ({best_rmse[1]} + {best_rmse[2]})")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path("logs").mkdir(exist_ok=True)
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚°è¿½åŠ 
    logger.add("logs/alternative_metrics_experiment_{time}.log", level="INFO")

    experiment = AlternativeMetricsExperiment()

    # å®Œå…¨å®Ÿé¨“ã‚’å®Ÿè¡Œï¼ˆå…¨ãƒ¡ãƒˆãƒªãƒƒã‚¯ Ã— å…¨ãƒ¢ãƒ‡ãƒ«ï¼‰
    results = experiment.run_full_experiment()

    logger.success("ä»£æ›¿è©•ä¾¡æŒ‡æ¨™å®Ÿé¨“å®Œäº†!")


if __name__ == "__main__":
    main()