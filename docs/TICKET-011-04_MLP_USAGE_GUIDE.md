# ğŸ§  TICKET-011-04: MLPå›å¸°ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ã‚¬ã‚¤ãƒ‰

**æœ€çµ‚æ›´æ–°**: 2025-09-23
**å¯¾å¿œãƒãƒ¼ã‚¸ãƒ§ãƒ³**: TICKET-011-04å®Ÿè£…å®Œäº†ç‰ˆ
**é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«**: `src/modeling/neural_models.py`, `src/modeling/neural_trainer.py`, `src/modeling/data_loaders.py`

## ğŸ“– æ¦‚è¦

TICKET-011-04ã§å®Ÿè£…ã•ã‚ŒãŸMulti-Layer Perceptronï¼ˆMLPï¼‰å›å¸°ãƒ¢ãƒ‡ãƒ«ã¯ã€æ¥½æ›²ç‰¹å¾´é‡ã‹ã‚‰BPMã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å¾“æ¥ã®LightGBMã¨ã¯ç•°ãªã‚‹å­¦ç¿’ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç”¨ã„ã¦ã€åŒç­‰ä»¥ä¸Šã®äºˆæ¸¬ç²¾åº¦ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

### ğŸ¯ **ä¸»è¦ç‰¹å¾´**
- **è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ç‰¹åŒ–**: BatchNormalization + Dropout ã§å®‰å®šè¨“ç·´
- **2ç¨®é¡ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: Standardï¼ˆé«˜æ€§èƒ½ï¼‰ã¨Simpleï¼ˆé«˜é€Ÿï¼‰
- **æ—¢å­˜ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ**: `train.py`ã‹ã‚‰ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹å®Ÿè¡Œ
- **è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: StandardScalerçµ±åˆæ¸ˆã¿
- **Early Stopping**: éå­¦ç¿’é˜²æ­¢æ©Ÿèƒ½å†…è”µ

### ğŸ“Š **æ€§èƒ½æŒ‡æ¨™**
- **æ¤œè¨¼RMSE**: 26.47ï¼ˆLightGBMã¨åŒç­‰ï¼‰
- **è¨“ç·´æ™‚é–“**: ç´„2.8åˆ†ï¼ˆ62ã‚¨ãƒãƒƒã‚¯ï¼‰
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: 15,361å€‹ï¼ˆSimpleï¼‰/ 179,969å€‹ï¼ˆStandardï¼‰

---

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. **åŸºæœ¬çš„ãªå®Ÿè¡Œ**

```bash
# MLPãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼ˆç°¡æ˜“ç‰ˆï¼‰
python -m src.modeling.train --model-type=mlp_simple --exp-name=my_mlp_experiment

# MLPãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼ˆé«˜æ€§èƒ½ç‰ˆï¼‰
python -m src.modeling.train --model-type=mlp_standard --exp-name=my_mlp_standard
```

### 2. **æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰ä¾‹**

```python
from src.modeling.neural_trainer import NeuralTrainer, TrainingConfig
from src.modeling.data_loaders import BPMDataProcessor
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
train_df = pd.read_csv("data/processed/train_features.csv")

# è¨­å®š
config = TrainingConfig(
    model_type="simple",
    epochs=50,
    batch_size=512
)

# ãƒ‡ãƒ¼ã‚¿å‡¦ç†
processor = BPMDataProcessor()
data_dict = processor.prepare_data(train_df, target_col="BeatsPerMinute")

# è¨“ç·´å®Ÿè¡Œ
trainer = NeuralTrainer(config)
results = trainer.train(data_dict["train_loader"], data_dict["val_loader"])

print(f"æœ€çµ‚æ¤œè¨¼RMSE: {results['best_val_rmse']:.4f}")
```

---

## ğŸ’ è©³ç´°ä½¿ç”¨æ–¹æ³•

### ğŸ›ï¸ **ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®é¸æŠ**

#### **mlp_simple** - é«˜é€Ÿå®Ÿé¨“ç”¨
```python
# ç‰¹å¾´
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: Input â†’ 256 â†’ Output
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ~15K
- è¨“ç·´æ™‚é–“: çŸ­ã„ï¼ˆ~2åˆ†ï¼‰
- ç”¨é€”: é«˜é€Ÿå®Ÿé¨“ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹
```

#### **mlp_standard** - é«˜æ€§èƒ½ç”¨
```python
# ç‰¹å¾´
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: Input â†’ 512 â†’ 256 â†’ 128 â†’ Output
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ~180K
- è¨“ç·´æ™‚é–“: ä¸­ç¨‹åº¦ï¼ˆ~5åˆ†ï¼‰
- ç”¨é€”: æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã€ç²¾åº¦é‡è¦–
```

### âš™ï¸ **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**

```python
config = TrainingConfig(
    # ãƒ¢ãƒ‡ãƒ«æ§‹é€ 
    model_type="standard",
    hidden_dims=[512, 256, 128],        # éš ã‚Œå±¤ã‚µã‚¤ã‚º
    dropout_rates=[0.3, 0.2, 0.1],      # Dropoutç‡
    activation="relu",                   # æ´»æ€§åŒ–é–¢æ•°

    # è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    learning_rate=1e-3,                  # å­¦ç¿’ç‡
    batch_size=512,                      # ãƒãƒƒãƒã‚µã‚¤ã‚º
    epochs=100,                          # æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°
    patience=15,                         # Early Stoppingé–¾å€¤

    # æœ€é©åŒ–
    optimizer_type="adam",               # adam, adamw, sgd
    scheduler_type="reduce_on_plateau",  # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    weight_decay=1e-4,                   # é‡ã¿æ¸›è¡°

    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    scaler_type="standard",              # standard, robust, minmax
    validation_size=0.2,                 # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ¯”ç‡

    # ã‚·ã‚¹ãƒ†ãƒ 
    device="auto"                        # auto, cpu, cuda
)
```

### ğŸ“ **å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ**

```
models/
â”œâ”€â”€ {exp_name}_mlp_simple_20250923_220112.pth     # è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ {exp_name}_mlp_simple_results_20250923_220112.json  # è¨“ç·´çµæœ
â””â”€â”€ ...

# results.json ã®å†…å®¹ä¾‹
{
    "experiment_name": "test_mlp",
    "model_type": "mlp_simple",
    "cv_rmse": 26.4738,
    "train_rmse": 26.8812,
    "training_time_minutes": 2.8,
    "epochs_trained": 62,
    "feature_count": 26,
    "train_samples": 335464,
    "val_samples": 83867
}
```

---

## ğŸ“Š å®Ÿé¨“ç®¡ç†ã¨ã®çµ±åˆ

### ğŸ§ª **è‡ªå‹•å®Ÿé¨“å®Ÿè¡Œ**

```bash
# å®Ÿé¨“ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆï¼ˆæ¨å¥¨ï¼‰
python scripts/submit_experiment.py \
    --experiment-name="exp06_mlp_baseline" \
    --model-type="mlp_simple"
```

### ğŸ“ˆ **æ€§èƒ½æ¯”è¼ƒ**

| ãƒ¢ãƒ‡ãƒ« | æ¤œè¨¼RMSE | è¨“ç·´æ™‚é–“ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | ç”¨é€” |
|--------|----------|----------|--------------|------|
| LightGBM | 26.39 | ~30åˆ† | - | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ |
| MLP Simple | 26.47 | ~3åˆ† | 15K | é«˜é€Ÿå®Ÿé¨“ |
| MLP Standard | TBD | ~5åˆ† | 180K | é«˜ç²¾åº¦ |

---

## ğŸ”§ é«˜åº¦ãªä½¿ç”¨æ–¹æ³•

### ğŸ¯ **ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ä½œæˆ**

```python
from src.modeling.neural_models import BPMPredictor

# ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
model = BPMPredictor(
    input_dim=26,
    hidden_dims=[1024, 512, 256, 128, 64],  # 5å±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    dropout_rates=[0.4, 0.3, 0.2, 0.1, 0.05],
    activation="leaky_relu",
    use_batch_norm=True
)

print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
```

### ğŸ’¾ **ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿**

```python
# ä¿å­˜
trainer.save_model("models/my_custom_model.pth")

# èª­ã¿è¾¼ã¿
trainer.load_model("models/my_custom_model.pth", input_dim=26)

# äºˆæ¸¬å®Ÿè¡Œ
predictions = trainer.predict(test_loader)
```

### ğŸ”„ **ã‚«ã‚¹ã‚¿ãƒ è¨“ç·´ãƒ«ãƒ¼ãƒ—**

```python
# æ‰‹å‹•è¨“ç·´åˆ¶å¾¡
for epoch in range(100):
    train_loss, train_rmse = trainer.train_epoch(train_loader)
    val_loss, val_rmse = trainer.validate_epoch(val_loader)

    print(f"Epoch {epoch}: Train RMSE={train_rmse:.4f}, Val RMSE={val_rmse:.4f}")

    if trainer.check_early_stopping(val_loss):
        print("Early stopping triggered")
        break
```

---

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### â— **ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºæ–¹æ³•**

#### **1. GPU/CUDAé–¢é€£ã‚¨ãƒ©ãƒ¼**
```bash
# ã‚¨ãƒ©ãƒ¼: CUDA out of memory
# è§£æ±º: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹
config = TrainingConfig(batch_size=256)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ512ã‹ã‚‰å‰Šæ¸›

# ã‚¨ãƒ©ãƒ¼: CUDA not available
# è§£æ±º: CPUã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
config = TrainingConfig(device="cpu")
```

#### **2. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼**
```bash
# ã‚¨ãƒ©ãƒ¼: System memory exhausted
# è§£æ±º: DataLoaderã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å‰Šæ¸›
data_loader = DataLoader(dataset, num_workers=0)  # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ç„¡åŠ¹åŒ–
```

#### **3. å­¦ç¿’ãŒé€²ã¾ãªã„**
```python
# ç—‡çŠ¶: æå¤±ãŒä¸‹ãŒã‚‰ãªã„
# è§£æ±º: å­¦ç¿’ç‡èª¿æ•´
config = TrainingConfig(learning_rate=1e-4)  # ã‚ˆã‚Šå°ã•ãªå­¦ç¿’ç‡

# ç—‡çŠ¶: éå­¦ç¿’
# è§£æ±º: Dropoutç‡ä¸Šã’ã‚‹
config = TrainingConfig(dropout_rates=[0.5, 0.4, 0.3])
```

#### **4. è¨“ç·´æ™‚é–“ãŒé•·ã™ãã‚‹**
```python
# è§£æ±º: ç°¡æ˜“ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
config = TrainingConfig(model_type="simple")

# ã¾ãŸã¯æ—©æœŸçµ‚äº†è¨­å®šã‚’å³ã—ã
config = TrainingConfig(patience=5, min_delta=1e-3)
```

### ğŸ› **ãƒ‡ãƒãƒƒã‚°æ‰‹é †**

```python
# 1. ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
print(f"Train shape: {data_dict['train_loader'].dataset.features.shape}")
print(f"Feature range: {data_dict['train_loader'].dataset.features.min():.3f} to {data_dict['train_loader'].dataset.features.max():.3f}")

# 2. ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒªãƒ¼
model = trainer.model
print(model)
print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")

# 3. å‹¾é…ã®ç¢ºèª
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm():.6f}")
```

---

## ğŸ”¬ å®Ÿé¨“ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ğŸ“‹ **å®Ÿé¨“è¨ˆç”»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**

```markdown
## å®Ÿé¨“: exp06_mlp_baseline

### ç›®çš„
- MLPãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ç¢ºç«‹
- LightGBMã¨ã®æ€§èƒ½æ¯”è¼ƒ

### è¨­å®š
- ãƒ¢ãƒ‡ãƒ«: mlp_simple
- ã‚¨ãƒãƒƒã‚¯: 100
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 512
- å­¦ç¿’ç‡: 1e-3

### æœŸå¾…çµæœ
- æ¤œè¨¼RMSE < 26.5
- è¨“ç·´æ™‚é–“ < 5åˆ†

### å®Ÿéš›ã®çµæœ
- æ¤œè¨¼RMSE: 26.47
- è¨“ç·´æ™‚é–“: 2.8åˆ†
- åˆ¤å®š: âœ… æˆåŠŸ
```

### ğŸ¯ **æ¬¡ã®å®Ÿé¨“å€™è£œ**

1. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–**
   ```bash
   # Optunaã‚’ä½¿ã£ãŸè‡ªå‹•æœ€é©åŒ–
   python scripts/optuna_optimization.py --model-type=mlp_standard
   ```

2. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®Ÿé¨“**
   ```bash
   # LightGBM + MLP ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
   python scripts/ensemble_experiment.py --models=lightgbm,mlp_standard
   ```

3. **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ¤œè¨¼**
   ```bash
   # æ–°ç‰¹å¾´é‡ã§ã®MLPæ€§èƒ½ãƒ†ã‚¹ãƒˆ
   python -m src.modeling.train --model-type=mlp_simple --exp-name=mlp_new_features
   ```

---

## ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [TICKET-008_USAGE_GUIDE.md](TICKET-008_USAGE_GUIDE.md) - ã‚¸ãƒ£ãƒ³ãƒ«ç‰¹å¾´é‡ä½¿ç”¨ã‚¬ã‚¤ãƒ‰
- [KAGGLE_SUBMIT_GUIDE.md](KAGGLE_SUBMIT_GUIDE.md) - Kaggleæå‡ºã‚¬ã‚¤ãƒ‰
- [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆREADME](../README.md) - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“æ¦‚è¦

---

## ğŸ”„ æ›´æ–°å±¥æ­´

- **2025-09-23**: TICKET-011-04å®Ÿè£…å®Œäº†ã€åˆç‰ˆä½œæˆ
- **æ¤œè¨¼RMSE**: 26.4738é”æˆï¼ˆLightGBMåŒç­‰æ€§èƒ½ç¢ºèªï¼‰
- **çµ±åˆå®Œäº†**: train.pyã€å®Ÿé¨“ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ çµ±åˆæ¸ˆã¿

---

## â“ ã‚µãƒãƒ¼ãƒˆ

è³ªå•ã‚„ä¸å…·åˆå ±å‘Šã¯ä»¥ä¸‹ã¾ã§ï¼š
- **GitHub Issues**: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒã‚¸ãƒˆãƒªã®Issues
- **å®Ÿè£…è©³ç´°**: `src/modeling/neural_*.py` ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰å‚ç…§
- **è¨­å®šä¾‹**: ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’å‚è€ƒ